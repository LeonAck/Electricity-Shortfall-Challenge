{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working directory: c:\\Users\\lackerman008\\OneDrive - pwc\\Outside\\Code\\Machine learning\\Electricity Shortfall Challenge\n",
      "Run name: shallow4\n",
      "Run ID: _20250721_214318\n",
      "Data laden...\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.chdir(\"c:/Users/lackerman008/OneDrive - pwc/Outside/Code/Machine learning/Electricity Shortfall Challenge\")\n",
    "\n",
    "print(f\"working directory: {os.getcwd()}\")\n",
    "\n",
    "\n",
    "\n",
    "from data_loading import load_data\n",
    "from config_and_logging import load_config, generate_run_id, save_run_metadata, create_output_dir, log_to_mlflow\n",
    "from model_pipeline import choose_best_model\n",
    "\n",
    "import joblib\n",
    "\n",
    "config_path = os.path.join('Configs', 'shallow4.yaml')\n",
    "\n",
    "config = load_config(config_path=config_path)\n",
    "run_name = config['run']['run_name']\n",
    "run_id = generate_run_id(config)\n",
    "output_dir = create_output_dir(run_name, run_id)\n",
    "\n",
    "target_column = config['data']['target_column']\n",
    "print(\"Run name:\", run_name)\n",
    "print(\"Run ID:\", run_id)\n",
    "print(\"Data laden...\")\n",
    "train_df, test_df, _ = load_data(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choose best models...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from plots import plot_predictions\n",
    "from models import train_ar_diff_model, predict_ar_diff, get_model\n",
    "from preprocessing import get_pipeline_for_model, create_preprocessing_pipeline, get_imputer\n",
    "from model_pipeline import split_data\n",
    "\n",
    "print(\"Choose best models...\")\n",
    "\n",
    "train_val_split = config['preprocessing']['train_val_split']\n",
    "\n",
    "best_rmse = float(\"inf\")\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_X_train = None\n",
    "best_pipeline = None\n",
    "\n",
    "y_train = train_df[config['data']['target_column']]\n",
    "X_train = train_df.drop(columns=[config['data']['target_column']])\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Bereken de Root Mean Squared Error\n",
    "    \"\"\"\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def get_pipeline_for_model(model, config):\n",
    "\n",
    "    if model[\"type\"] in [\"AR\", \"MA\", \"SMA\"]:\n",
    "        return None  # TODO pipeline to be added\n",
    "    \n",
    "    elif model.get('scaling', None):\n",
    "        return create_preprocessing_pipeline(imputer=get_imputer(config), \n",
    "                                             freq=config['preprocessing']['freq'],\n",
    "                                             fill_method=config['preprocessing']['fill_method'], \n",
    "                                             add_time_dummies=config['preprocessing']['add_time_dummies'], \n",
    "                                             scaling=model['scaling'])\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"No preprocessing pipeline defined for model_type: {model['type']}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'LinearRegression', 'params': {}, 'scaling': True}, {'type': 'RandomForest', 'params': {'n_estimators': 10, 'random_state': 42}, 'scaling': False}, {'type': 'Ridge', 'params': {'alpha': 10.0, 'random_state': 42}, 'scaling': True}, {'type': 'Lasso', 'params': {'alpha': 1.0, 'random_state': 42}, 'scaling': True}, {'type': 'ElasticNet', 'params': {'alpha': 1.0, 'l1_ratio': 0.5, 'random_state': 42}, 'scaling': True}, {'type': 'RandomForest', 'params': {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}, 'scaling': False}, {'type': 'ExtraTreesRegressor', 'params': {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}, 'scaling': False}, {'type': 'GradientBoostingRegressor', 'params': {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'random_state': 42}, 'scaling': False}, {'type': 'XGBRegressor', 'params': {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'random_state': 42}, 'scaling': False}, {'type': 'KNeighborsRegressor', 'params': {'n_neighbors': 8}, 'scaling': True}, {'type': 'SVR', 'params': {'kernel': 'rbf', 'C': 10.0, 'gamma': 'scale'}, 'scaling': True}, {'type': 'MA1', 'params': {'window': 1, 'difference_order': 1}, 'scaling': False}, {'type': 'AR', 'params': {'lags': 1, 'difference_order': 1}, 'scaling': False}]\n"
     ]
    }
   ],
   "source": [
    "print(config['models'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'type': 'LinearRegression', 'params': {}, 'scaling': True}\n",
      "LinearRegression\n",
      "{'type': 'RandomForest', 'params': {'n_estimators': 10, 'random_state': 42}, 'scaling': False}\n",
      "RandomForest\n",
      "{'type': 'Ridge', 'params': {'alpha': 10.0, 'random_state': 42}, 'scaling': True}\n",
      "Ridge\n",
      "{'type': 'Lasso', 'params': {'alpha': 1.0, 'random_state': 42}, 'scaling': True}\n",
      "Lasso\n",
      "{'type': 'ElasticNet', 'params': {'alpha': 1.0, 'l1_ratio': 0.5, 'random_state': 42}, 'scaling': True}\n",
      "ElasticNet\n",
      "{'type': 'RandomForest', 'params': {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}, 'scaling': False}\n",
      "RandomForest\n",
      "{'type': 'ExtraTreesRegressor', 'params': {'n_estimators': 100, 'max_depth': 10, 'random_state': 42}, 'scaling': False}\n",
      "ExtraTreesRegressor\n",
      "{'type': 'GradientBoostingRegressor', 'params': {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'random_state': 42}, 'scaling': False}\n",
      "GradientBoostingRegressor\n",
      "{'type': 'XGBRegressor', 'params': {'n_estimators': 100, 'max_depth': 4, 'learning_rate': 0.1, 'random_state': 42}, 'scaling': False}\n",
      "XGBRegressor\n",
      "{'type': 'KNeighborsRegressor', 'params': {'n_neighbors': 8}, 'scaling': True}\n",
      "KNeighborsRegressor\n",
      "{'type': 'SVR', 'params': {'kernel': 'rbf', 'C': 10.0, 'gamma': 'scale'}, 'scaling': True}\n",
      "SVR\n",
      "{'type': 'MA1', 'params': {'window': 1, 'difference_order': 1}, 'scaling': False}\n",
      "MA1\n",
      "{'type': 'AR', 'params': {'lags': 1, 'difference_order': 1}, 'scaling': False}\n",
      "AR\n"
     ]
    }
   ],
   "source": [
    "for model in config['models']:\n",
    "    print(model)\n",
    "    print(model[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearRegression\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not dict",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[63]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m config[\u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m      4\u001b[39m         \u001b[38;5;28mprint\u001b[39m(model[\u001b[33m'\u001b[39m\u001b[33mtype\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m         pipeline = \u001b[43mget_pipeline_for_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m         X_train_processed, y_train_processed = pipeline.fit_transform(X_train, y_train)\n\u001b[32m      9\u001b[39m         X_train_new, X_val, y_train_new, y_val = split_data(X_train_processed, y_train_processed, train_val_split)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\lackerman008\\OneDrive - pwc\\Outside\\Code\\Machine learning\\Electricity Shortfall Challenge\\preprocessing.py:23\u001b[39m, in \u001b[36mget_pipeline_for_model\u001b[39m\u001b[34m(model_type, config)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_type \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mAR\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mMA\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mSMA\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m     21\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# TODO pipeline to be added\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m]\u001b[49m[\u001b[33m'\u001b[39m\u001b[33mscaling\u001b[39m\u001b[33m'\u001b[39m]:\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m create_preprocessing_pipeline(imputer=get_imputer(config), \n\u001b[32m     25\u001b[39m                                          freq=config[\u001b[33m'\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mfreq\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     26\u001b[39m                                          fill_method=config[\u001b[33m'\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mfill_method\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     27\u001b[39m                                          add_time_dummies=config[\u001b[33m'\u001b[39m\u001b[33mpreprocessing\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33madd_time_dummies\u001b[39m\u001b[33m'\u001b[39m], \n\u001b[32m     28\u001b[39m                                          scaling=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     29\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m config[\u001b[33m'\u001b[39m\u001b[33mmodels\u001b[39m\u001b[33m'\u001b[39m][model_type][\u001b[33m'\u001b[39m\u001b[33mscaling\u001b[39m\u001b[33m'\u001b[39m]:\n",
      "\u001b[31mTypeError\u001b[39m: list indices must be integers or slices, not dict"
     ]
    }
   ],
   "source": [
    "from preprocessing import get_pipeline_for_model\n",
    "\n",
    "for model in config['models']:\n",
    "        print(model['type'])\n",
    "        pipeline = get_pipeline_for_model(model, config)\n",
    "        \n",
    "        X_train_processed, y_train_processed = pipeline.fit_transform(X_train, y_train)\n",
    "\n",
    "        X_train_new, X_val, y_train_new, y_val = split_data(X_train_processed, y_train_processed, train_val_split)\n",
    "\n",
    "        model_type = get_model(model['type'], model['params'])\n",
    "        trained_model = model_type.fit(X_train_new, y_train_new)\n",
    "\n",
    "        # Predict, evaluate and plot\n",
    "        predictions = trained_model.predict(X_val)\n",
    "        rmse = evaluate_model(y_val, predictions)\n",
    "        plot_predictions(y_val, predictions, model['type'], output_dir, dataset_name=\"validation\")\n",
    "        \n",
    "        print(f\"Model: {model['type']}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "        if rmse < best_rmse:\n",
    "            best_rmse = rmse\n",
    "            best_model = model\n",
    "            best_model_name = model['type']\n",
    "            best_X_train = X_train\n",
    "            best_pipeline = pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
