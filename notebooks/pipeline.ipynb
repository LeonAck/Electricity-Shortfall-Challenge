{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'data_loading'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdata_loading\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_data\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconfig_and_logging\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_config, generate_run_id, save_run_metadata, create_output_dir, log_to_mlflow\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmodel_pipeline\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m choose_best_model, train_full_model_predict_test_set\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'data_loading'"
     ]
    }
   ],
   "source": [
    "from data_loading import load_data\n",
    "from config_and_logging import load_config, generate_run_id, save_run_metadata, create_output_dir, log_to_mlflow\n",
    "from model_pipeline import choose_best_model, train_full_model_predict_test_set\n",
    "from models import get_model\n",
    "from preprocessing import get_imputer, create_preprocessing_pipeline\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from plots import plot_predictions\n",
    "from models import train_ar_diff_model, predict_ar_diff\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m config_path = \u001b[33m'\u001b[39m\u001b[33mConfigs/shallow2_scaling_timeseriessplit.yaml\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m config = \u001b[43mload_config\u001b[49m(config_path=os.path.join(os.getcwd(), config_path))\n\u001b[32m      4\u001b[39m run_name = config[\u001b[33m'\u001b[39m\u001b[33mrun\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      5\u001b[39m run_id = generate_run_id(config)\n",
      "\u001b[31mNameError\u001b[39m: name 'load_config' is not defined"
     ]
    }
   ],
   "source": [
    "config_path = 'Configs/shallow2_scaling_timeseriessplit.yaml'\n",
    "\n",
    "config = load_config(config_path=os.path.join(os.getcwd(), config_path))\n",
    "run_name = config['run']['run_name']\n",
    "run_id = generate_run_id(config)\n",
    "output_dir = create_output_dir(run_name, run_id)\n",
    "\n",
    "target_column = config['data']['target_column']\n",
    "print(\"Run name:\", run_name)\n",
    "print(\"Run ID:\", run_id)\n",
    "print(\"Data laden...\")\n",
    "train_df, test_df, sample_submission = load_data(config)\n",
    "\n",
    "print(\"Pipelines aanmaken...\")\n",
    "\n",
    "# Shared preprocessing config\n",
    "imputer = get_imputer(config)\n",
    "freq = config['preprocessing']['freq']\n",
    "fill_method = config['preprocessing']['fill_method']\n",
    "add_time_dummies = config['preprocessing']['add_time_dummies']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lackerman008\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\lackerman008\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Split X and y sets\n",
    "y_train = train_df[target_column]\n",
    "X_train = train_df.drop(columns=[target_column])\n",
    "\n",
    "# Pipelines\n",
    "pipeline_scaled = create_preprocessing_pipeline(imputer, freq, fill_method, add_time_dummies, scaling=True)\n",
    "pipeline_no_scaling = create_preprocessing_pipeline(imputer, freq, fill_method, add_time_dummies, scaling=False)\n",
    "\n",
    "# Fit both pipelines on training data\n",
    "X_train_scaled = pipeline_scaled.fit_transform(X_train)\n",
    "X_train_no_scaling = pipeline_no_scaling.fit_transform(X_train)\n",
    "\n",
    "# Transform test set as well (will be needed later)\n",
    "test_scaled = pipeline_scaled.transform(test_df)\n",
    "test_no_scaling = pipeline_no_scaling.transform(test_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8763,)\n",
      "(8763, 51)\n",
      "(8763, 51)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(y_train.shape)\n",
    "print(X_train_scaled.shape)\n",
    "print(X_train_no_scaling.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before 2068\n",
      "[Timestamp('2015-01-05 15:00:00'), Timestamp('2015-01-05 18:00:00'), Timestamp('2015-02-01 15:00:00'), Timestamp('2015-02-01 18:00:00')]\n",
      "DatetimeIndex(['2015-01-05 15:00:00', '2015-01-05 18:00:00',\n",
      "               '2015-02-01 15:00:00', '2015-02-01 18:00:00'],\n",
      "              dtype='datetime64[ns]', freq=None)\n",
      "after 2068\n",
      "(8763, 48)\n",
      "(8763,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lackerman008\\AppData\\Local\\Temp\\ipykernel_31232\\1770656949.py:25: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='3H')\n",
      "C:\\Users\\lackerman008\\AppData\\Local\\Temp\\ipykernel_31232\\1770656949.py:35: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  df = df.asfreq(freq)\n"
     ]
    }
   ],
   "source": [
    "# Convert Valencia_wind_deg to numerical values if it exists\n",
    "if 'Valencia_wind_deg' in train_df.columns:\n",
    "    print(\"Converting Valencia_wind_deg to numerical values...\")\n",
    "    train_df['Valencia_wind_deg_cat'] = train_df['Valencia_wind_deg'].astype(str).str.replace('level_', '').astype(float)\n",
    "    train_df = train_df.drop(columns=['Valencia_wind_deg'])\n",
    "\n",
    "# Convert Seville_pressure to numerical values if it exists\n",
    "if 'Seville_pressure' in train_df.columns:\n",
    "    print(\"Converting Seville_pressure to numerical values...\")\n",
    "    train_df['Seville_pressure_cat'] = train_df['Seville_pressure'].astype(str).str.replace('sp', '').astype(float)\n",
    "    train_df = train_df.drop(columns=['Seville_pressure'])\n",
    "\n",
    "\n",
    "print(\"before\", train_df.isna().sum().sum())   \n",
    "def set_datetime_as_index(df, fill_method='interpolate', freq='3H'):\n",
    "        \"\"\"\n",
    "        Convert 'time' column to datetime index, reindex to regular intervals,\n",
    "        and impute missing rows.\n",
    "        \"\"\"\n",
    "        # Convert and sort time\n",
    "        df['time'] = pd.to_datetime(df['time'])\n",
    "        df = df.set_index('time')\n",
    "\n",
    "        # Create complete datetime index\n",
    "        full_index = pd.date_range(start=df.index.min(), end=df.index.max(), freq='3H')\n",
    "        print([dat for dat in full_index if dat not in df.index])  # Check the difference between the first index and the full index\n",
    "        # Reindex\n",
    "        df = df.reindex(full_index)\n",
    "\n",
    "\n",
    "        # Identify fully missing rows\n",
    "        fully_missing_mask = df.isna().all(axis=1)\n",
    "        print(df.index[fully_missing_mask])\n",
    "           \n",
    "        df = df.asfreq(freq)\n",
    "\n",
    "        df.drop(index=df.index[fully_missing_mask], inplace=True)\n",
    "        return df\n",
    "\n",
    "# Ensure time is datetime type\n",
    "if 'time' in train_df.columns:\n",
    "    train_df_index = set_datetime_as_index(train_df)\n",
    "\n",
    "print(\"after\", train_df_index.isna().sum().sum()) \n",
    "print(train_df_index.shape)\n",
    "print(y_train.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load models\n",
    "model_cfgs = config['models']\n",
    "models_to_try = {}\n",
    "\n",
    "for mc in model_cfgs:\n",
    "    model_name = mc['type']\n",
    "    scaling_needed = mc.get('scaling', False)\n",
    "\n",
    "    if scaling_needed:\n",
    "        X_train_transformed = X_train_scaled\n",
    "        X_test_transformed = test_scaled\n",
    "    else:\n",
    "        X_train_transformed = X_train_no_scaling\n",
    "        X_test_transformed = test_no_scaling\n",
    "\n",
    "    model = get_model(model_name, mc['params'])\n",
    "\n",
    "    models_to_try[model_name] = {\n",
    "        'model': model,\n",
    "        'X_train': X_train_transformed.copy(),\n",
    "        'X_test': X_test_transformed.copy()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model: MA1 with 7010 training samples and 1753 validation samples\n",
      "Model: MA1, RMSE: 5563.9103\n",
      "Training model: MA2 with 7010 training samples and 1753 validation samples\n",
      "Model: MA2, RMSE: 5602.8634\n",
      "Training model: SMA with 7010 training samples and 1753 validation samples\n",
      "Model: SMA, RMSE: 5517.9642\n"
     ]
    }
   ],
   "source": [
    "from model_pipeline import evaluate_model\n",
    "\n",
    "def train_and_evaluate_model(output_dir, model, model_name, X_train, y_train, X_val, y_val):\n",
    "    \"\"\" Train en evalueer een model op de validatieset. \"\"\"\n",
    "\n",
    "    if model_name == \"AR1\":\n",
    "        model_fit, last_value, lags = train_ar_diff_model(y_train)\n",
    "        predictions = predict_ar_diff(model_fit, last_value, lags, steps=len(y_val), index=y_val.index)\n",
    "        \n",
    "    elif model_name in [\"MA1\", \"MA2\", \"SMA\"]:\n",
    "        predictions = train_and_predict_ma(model_name, y_train, len(y_val), y_val.index if isinstance(y_val, pd.Series) else None)\n",
    "        \n",
    "    else:\n",
    "        model.fit(X_train, y_train.values)\n",
    "        predictions = model.predict(X_val)\n",
    "\n",
    "    # Bereken RMSE\n",
    "    rmse = evaluate_model(y_val, predictions)\n",
    "\n",
    "    plot_predictions(y_val, predictions, model_name, output_dir, dataset_name=\"validation\")\n",
    "\n",
    "    return model, model_name, rmse, predictions\n",
    "\n",
    "def train_and_predict_ma(model_name, y_train, prediction_steps, y_val_index=None):\n",
    "    \"\"\"\n",
    "    Train and predict using Moving Average models\n",
    "\n",
    "    Args:\n",
    "        model_name: 'MA1', 'MA2', or 'SMA'\n",
    "        y_train: Training time series\n",
    "        prediction_steps: Number of steps to predict\n",
    "        y_val_index: Index for predictions\n",
    "    \"\"\"\n",
    "    predictions = []\n",
    "\n",
    "    if model_name == 'MA1':\n",
    "        # MA(1) - uses last residual\n",
    "        # For simplicity, using naive implementation\n",
    "        last_value = y_train.iloc[-1]\n",
    "        for _ in range(prediction_steps):\n",
    "            pred = last_value  # Simplified MA(1)\n",
    "            predictions.append(pred)\n",
    "            \n",
    "    elif model_name == 'MA2':\n",
    "        # MA(2) - uses last 2 residuals\n",
    "        last_values = y_train.tail(2).mean()\n",
    "        for _ in range(prediction_steps):\n",
    "            pred = last_values  # Simplified MA(2)\n",
    "            predictions.append(pred)\n",
    "            \n",
    "    elif model_name == 'SMA':\n",
    "        # Simple Moving Average\n",
    "        window = min(8, len(y_train))  # 24-hour window or available data\n",
    "        sma_value = y_train.tail(window).mean()\n",
    "        predictions = [sma_value] * prediction_steps\n",
    "\n",
    "    # Convert to pandas Series with proper index\n",
    "    if y_val_index is not None:\n",
    "        return pd.Series(predictions, index=y_val_index)\n",
    "    else:\n",
    "        return np.array(predictions)\n",
    "\n",
    "train_val_split = config['preprocessing']['train_val_split']\n",
    "\n",
    "best_rmse = float(\"inf\")\n",
    "best_model = None\n",
    "best_model_name = \"\"\n",
    "best_X_train = None\n",
    "best_X_test = None\n",
    "\n",
    "for model_name, entry in models_to_try.items():\n",
    "    if model_name in ['LinearRegression', 'RandomForest', 'Ridge', 'Lasso', 'ElasticNet', 'BayesianRidge', \n",
    "                      'SGDRegressor', 'ExtraTreesRegressor', 'GradientBoostingRegressor', 'XGBRegressor', \n",
    "                      'KNeighborsRegressor', 'SVR']:\n",
    "        continue\n",
    "        \n",
    "\n",
    "    model = entry['model']\n",
    "    X_train = entry['X_train']\n",
    "    X_test = entry['X_test']\n",
    "    \n",
    "    # Define split point\n",
    "    train_val_loc = int(len(X_train) * (1-train_val_split))\n",
    "\n",
    "    # Split\n",
    "    X_train_new = X_train[:train_val_loc]\n",
    "    X_val = X_train[train_val_loc:]\n",
    "\n",
    "    y_train_new = y_train.iloc[:train_val_loc]\n",
    "    y_val = y_train.iloc[train_val_loc:]\n",
    "\n",
    "    print(f\"Training model: {model_name} with {X_train_new.shape[0]} training samples and {X_val.shape[0]} validation samples\")\n",
    "\n",
    "    # Here: simple train on X_train, evaluate on same data (adjust to proper CV or split if needed)\n",
    "    model, model_name, rmse, predictions = train_and_evaluate_model(\n",
    "        output_dir, model, model_name, X_train_new, y_train_new, X_val, y_val\n",
    "    )\n",
    "    \n",
    "    print(f\"Model: {model_name}, RMSE: {rmse:.4f}\")\n",
    "\n",
    "    if rmse < best_rmse:\n",
    "        best_rmse = rmse\n",
    "        best_model = model\n",
    "        best_model_name = model_name\n",
    "        best_X_train = X_train\n",
    "        best_X_test = X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/07/12 23:04:32 WARNING mlflow.models.model: `artifact_path` is deprecated. Please use `name` instead.\n",
      "2025/07/12 23:04:33 WARNING mlflow.sklearn: Model was missing function: predict. Not logging python_function flavor!\n",
      "2025/07/12 23:04:44 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Voorspellingen opgeslagen in 'sample_submission.csv'\n"
     ]
    }
   ],
   "source": [
    "def train_full_model_predict_test_set(best_model, X_train, X_test, y_train):\n",
    "\n",
    "    if best_model == \"AutoReg\":  # Voor AR1 modellen hebben we alleen y_train nodig\n",
    "        model_fit, last_value, lags = train_ar_diff_model(y_train)\n",
    "        test_predictions = predict_ar_diff(model_fit, last_value, lags, steps=len(X_test), index=X_test.index if isinstance(X_test, pd.DataFrame) else None)\n",
    "\n",
    "    elif model_name in [\"MA1\", \"MA2\", \"SMA\"]:\n",
    "        test_predictions = train_and_predict_ma(model_name, y_train, len(X_test), X_test.index if isinstance(X_test, pd.DataFrame) else None)\n",
    "\n",
    "    else:  # Voor andere modellen gebruiken we zowel X_train als y_train\n",
    "        best_model.fit(X_train, y_train.values)\n",
    "        test_predictions = best_model.predict(X_test)\n",
    "\n",
    "    return test_predictions\n",
    "\n",
    "metrics = {\"rmse_validation\": best_rmse, \"model\": best_model_name}\n",
    "\n",
    "save_run_metadata(output_dir, config, metrics)\n",
    "\n",
    "# Log to MLflow\n",
    "\n",
    "log_to_mlflow(config, output_dir, run_id, best_model_name, best_model, metrics, parameters=config.get(\"models\", {}))\n",
    "\n",
    "if True:\n",
    "    # Train on full set and predict on test set\n",
    "    test_predictions = train_full_model_predict_test_set(\n",
    "        best_model, \n",
    "        best_X_train, \n",
    "        best_X_test, \n",
    "        y_train\n",
    "    )\n",
    "\n",
    "    submission_df = pd.DataFrame({\n",
    "        'time': test_df.index,  # or test_df['time'] if that's your column\n",
    "        'load_shortfall_3h': test_predictions\n",
    "    })\n",
    "    submission_df.to_csv('sample_submission.csv', index=False)\n",
    "    print(\"\\nVoorspellingen opgeslagen in 'sample_submission.csv'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
